{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入套件 和定義函式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dcard 標籤要多注意，因為很常被更換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import urllib.request as req\n",
    "import numpy as np\n",
    "from scipy.linalg import norm\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import bs4, cloudscraper, random, time, requests\n",
    "\n",
    "# 載入 Selenium 相關模組\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "\n",
    "def tfidf_similarity(s1, s2):\n",
    "    def add_space(s):\n",
    "        return ' '.join(list(s))\n",
    "    # 將字中間加入空格\n",
    "    s1, s2 = add_space(s1), add_space(s2)\n",
    "    # 轉化為TF矩陣\n",
    "    cv = TfidfVectorizer(tokenizer=lambda s: s.split())\n",
    "    corpus = [s1, s2]\n",
    "    vectors = cv.fit_transform(corpus).toarray()\n",
    "    \n",
    "    # 計算TF係數\n",
    "    fiend = np.dot(vectors[0], vectors[1]) / (norm(vectors[0]) * norm(vectors[1])) \n",
    "    np.seterr(invalid='ignore') # 當計算結果為無意義(分母為0)，忽略此警告\n",
    "    return fiend\n",
    "    \n",
    "def stopwordslist(filepath):\n",
    "    stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]\n",
    "    return stopwords\n",
    "\n",
    "def movestopwords(sentence):\n",
    "    stopwords = stopwordslist('stopword.txt')  # 這裏加載停用詞的路徑\n",
    "    outstr = ''\n",
    "    for word in sentence:           \n",
    "        if word not in stopwords:  \n",
    "            if word != '\\t'and'\\n':\n",
    "                outstr += word\n",
    "    return outstr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定Chrome Driver 的執行檔路徑\n",
    "options = Options()\n",
    "options.add_argument(\"--incognito\") # 啟動進入無痕模式\n",
    "options.add_argument(\"--window-size=1,1\") # 頁面長度寬度調整\n",
    "# chrome_options.add_argument('--headless')  # 啟動Headless 無頭(隱藏瀏覽器)\n",
    "\n",
    "# 隱藏\"Chrome正在受到自動軟體的控制\"\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "options.add_argument('--disable-gpu') #關閉GPU 避免某些系統或是網頁出錯\n",
    "options.add_argument('--hide-scrollbars')     # 隱藏滾動條, 應對一些特殊頁面\n",
    "options.chrome_executable_path = \"C:\\\\Users\\\\student\\\\Desktop\\\\model_compar\\\\chromedriver.exe\"\n",
    "\n",
    "def dcardCraw(url):\n",
    "    #建立 Driver 物件實體，用程式操作瀏覽器運作\n",
    "    driver = webdriver.Chrome(options = options)\n",
    "    driver.minimize_window() #視窗縮小化\n",
    "    driver.get(url)\n",
    "    # print(driver.page_source)\n",
    "    data = driver.page_source #取得網頁的原始碼\n",
    "\n",
    "    # 讓beautifulSoup協助我們解析HTML格式文件\n",
    "    root = bs4.BeautifulSoup(data, \"html.parser\")\n",
    "    titles = root.find(\"div\", class_=\"sc-ba53eaa8-0 hKkUKs\")  # 用列表顯示全部爬蟲下來的標題\n",
    "\n",
    "    for title in titles:\n",
    "        result = title.text.strip().replace('\\n', '').replace(' ', '')\n",
    "        #印出內文\n",
    "        print(result)\n",
    "        \n",
    "    driver.close()\n",
    "    \n",
    "    return result\n",
    "    # titles代表div標籤\n",
    "    # 尋找class = \"title\" 的div 標籤，因為class是保留字，所以寫成class_\n",
    "    # root 代表整個網頁、title是網頁標籤也是網頁標題\n",
    "    # cls 是清空終端機(terminal)\n",
    "    # mode = \"a\"是以附加的方式打開並寫入文件，因為mode = \"w\"會將檔案清空在寫入，mode=\"a\"不會清空\n",
    "    \n",
    "def pttCraw(url):\n",
    "    #建立一個Request 物件，附加Request Headers 的資訊\n",
    "    request = req.Request(url, headers={\n",
    "        \"cookie\":\"over18=1\",\n",
    "        \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36\"\n",
    "    })\n",
    "    with req.urlopen(request) as response:\n",
    "        data = response.read().decode(\"utf-8\")\n",
    "\n",
    "    # print(data)\n",
    "    #「解析」原始碼，取得每篇文章的問題\n",
    "    # utf-8(比較省空間)有部分的漢字不能轉換所以要用GB18030編碼\n",
    "\n",
    "    root = bs4.BeautifulSoup(data, \"html.parser\") # 讓beautifulSoup協助我們解析HTML格式文件\n",
    "    titles = root.find(\"div\", class_ = \"bbs-screen bbs-content\").text # 用爬蟲抓內文\n",
    "    \n",
    "    #去除掉 target_content\n",
    "    target_content = '※ 發信站: 批踢踢實業坊(ptt.cc),'\n",
    "    content = titles.split(target_content)\n",
    "    \n",
    "    #去除掉 作者 看板 標題 時間\n",
    "    results = root.select('span.article-meta-value')\n",
    "\n",
    "    if len(results)>3:\n",
    "        #作者 看板 標題 時間\n",
    "        firstLine = \"作者\" + results[0].text + \"看板\" + results[1].text + \"標題\" + results[2].text + \"時間\" + results[3].text\n",
    "\n",
    "    content = content[0].split(firstLine)\n",
    "    \n",
    "    #去除掉文末 --\n",
    "    main_content = content[1].replace('--', '')\n",
    "\n",
    "    #去除掉換行\n",
    "    main_content = main_content.replace('\\n', '')\n",
    "    \n",
    "    #印出內文\n",
    "    print(main_content)\n",
    "    \n",
    "    return main_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### youtube API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoutubeSpider():\n",
    "    def __init__(self, api_key):\n",
    "        self.base_url = \"https://www.googleapis.com/youtube/v3/search?type=video&part=snippet&maxResults=1\"\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def get_html_to_json(self, path):\n",
    "        \"\"\"組合 URL 後 GET 網頁並轉換成 JSON\"\"\"\n",
    "        api_url = f\"{self.base_url}&key={self.api_key}{path}\"\n",
    "        r = requests.get(api_url)\n",
    "        if r.status_code == requests.codes.ok:\n",
    "            data = r.json()\n",
    "        else:\n",
    "            data = None\n",
    "        return data\n",
    "    \n",
    "    def get_ytSearch(self, theKey):\n",
    "        path = f'&q={theKey}'\n",
    "        data = self.get_html_to_json(path)\n",
    "        \n",
    "        try:\n",
    "            uploads_id = data['items'][0]['id']['videoId']\n",
    "            # uploads_id = data #輸出是dict\n",
    "        except KeyError:\n",
    "            uploads_id = None\n",
    "        return uploads_id\n",
    "    \n",
    "YOUTUBE_API_KEY = \"AIzaSyBKdJO0Q7tS8jQyuZUx0kNmgFD2L73Bn1E\"\n",
    "youtube_spider = YoutubeSpider(YOUTUBE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算配適度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('done_2021-08to12.csv')\n",
    "\n",
    "def find_song(url):\n",
    "    if url[12:15] == \"dca\":\n",
    "        article = dcardCraw(url)\n",
    "    else:\n",
    "        article = pttCraw(url)\n",
    "\n",
    "    lyrics=train['lyrics']\n",
    "    i=0\n",
    "    num=0\n",
    "    highpri=0\n",
    "    for text in lyrics:\n",
    "        text=movestopwords(text)\n",
    "        text=text.replace(' ','')\n",
    "        text=text.replace(',','，')\n",
    "        if tfidf_similarity(text, article)>highpri:\n",
    "            highpri=tfidf_similarity(text, article)\n",
    "            num=i\n",
    "        i+=1\n",
    "    \n",
    "    author = train.singer.iloc[num]\n",
    "    songName = train.name.iloc[num]\n",
    "    youtube_theKey = author + ' ' + songName  # 孫凱旋 專屬\n",
    "    uploads_id = youtube_spider.get_ytSearch(youtube_theKey)\n",
    "    \n",
    "    print('配適度:',highpri,'作者:',train.singer.iloc[num],'歌名:',train.name.iloc[num], '情緒:',train.moodCat.iloc[num])\n",
    "    print(\"連結:https://www.youtube.com/watch?v=\"+uploads_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 抓取檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自從大學二年級被診斷出自己有憂鬱症後，一路走來，從吃藥、藥量加重，到現在停藥，不只是身體上、心靈上，甚至想法上，都有不同的體悟。我想分享自己一路以來的心路歷程，希望讓正在被憂憂煩惱的捧油，可以明白你並不孤單，你熬過一次又一次的黑暗，都將使你變得更強大，當我們再次回過頭，會發現，我們所能承受的，遠遠大於過去的自己，因為在不知不覺中，我們都變得更強大了！----------------------------------------------------------------------其實在高中時期，自己曾經有過ig小帳，沒辦小帳還好，辦了小帳後，才發現原來自己可以這麼的負面，原來自己一直以來都這麼壓抑，在家人的眼裡，我似乎是非常陽光、樂觀的小孩，在朋友眼中我也是個愛笑的女孩吧！原來我以為的自己，其實並非是內心真正的自己。上了大學之後，我知道自己的內心依然很負面，但是不知道為什麼，我總是可以在家人、朋友在的時候，表現的一副好像自己很開心，好像一副“我很好”的樣子，當一個人獨處的時候，卻又覺得好累、好想哭，很討厭這樣的自己，為什麼我總是不快樂，為什麼快樂是這麼的奢侈，會對自己產生很多的懷疑，很多的為什麼，而最大的疑問，是“為什麼我總是無法讓自己開心起來？為什麼我總是無法改變這樣的狀態？”一次又一次的，讓我在憂鬱症發作的時候感到無助、無力，我每天吃不下、睡不著，甚至開始懷疑自己存在的價值。後來我去看了醫生，想要給自己一個改變的機會，但是由於服藥的關係，沒吃安眠藥睡不著，但是吃了以後隔天又無法正常起床，我有一個學期都沒有好好的到校上課，時常請假，我不得不把自己的身體狀態告訴家人，而當我真正向家人坦誠自己的狀態後，我開始發覺，其實有時候向別人承認自己過得不好，不是一件丟臉的事情，向別人承認自己不快樂、不開心也不是一件錯誤的事，或許，想要讓自己好起來的不二法門，就是正視它、面對它，勇敢接受這樣不完美的自己。所以後來我果斷停藥，剛停藥的時候身體非常的不舒服，每天都很想吐、吃不下，頭也很暈，甚至有一次還突然昏倒，儘管身體非常的不舒服，我還是不想再回到依賴藥物的生活了！因為藥物治標不治本，或許我可以因為吃藥而睡著，但是每次醒來的時候，身體都是沉重且疲憊，或許我可以因為吃藥而控制憂鬱的症狀，但我依然感受不到快樂，所以藥物終究只能暫時輔助、舒緩症狀，但真正想要痊癒，還是要靠自己。現在的我不會再排斥每一個憂鬱的到來，喜是一個情緒，你會有開心的時候，但是你並不會永遠只有開心，或許憂鬱也是同理，我們不會永遠的憂鬱，它終究是一個會消失的情緒吧！當憂鬱來臨的時候，我開始慢慢試著接受它、面對它，我會認真的難過，好好的哭一哭，把心理覺得沉重的所有事情認真發洩出來，之後才有辦法好好迎接開心的情緒，對吧！我想說，或許我們有時感到不被理解、不被認同，但是唯有我們勇敢的正視每一個自己，接受這樣不完美的自己，我們也才能慢慢被這個世界包容吧！或許有時候我們需要的不是一個理解我們的人，我們需要的是能夠接受不同的自己，與愛每一個自己的那份勇氣❤️\n",
      "\n",
      "放上一張溫馨的照片做結尾🥰謝謝大家耐心看完🌹\n",
      "配適度: 0.1460622154969947 作者: 羊羊 歌名: 溫暖 情緒: 愛\n",
      "連結:https://www.youtube.com/watch?v=lRFEz3m62pc\n"
     ]
    }
   ],
   "source": [
    "# article = find_song('https://www.dcard.tw/f/relationship/p/238632575')\n",
    "article = find_song('https://www.dcard.tw/f/talk/p/239984330')\n",
    "# article = find_song('https://www.dcard.tw/f/photography/p/240167044')\n",
    "# article = find_song(\"http://www.ptt.cc/bbs/Boy-Girl/M.1664277279.A.9AA.html\")\n",
    "# article = find_song(\"https://www.ptt.cc/bbs/Gossiping/M.1664530650.A.4E3.html\")\n",
    "# article = find_song(\"http://www.ptt.cc/bbs/Boy-Girl/M.1660356781.A.365.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9bf8e5f6ae5a440c6649c43ab49956741af2ee52909e232ddcd4abcc58504c21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
