{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入套件 和定義函式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dcard 標籤要多注意，因為很常被更換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import urllib.request as req\n",
    "import numpy as np\n",
    "from scipy.linalg import norm\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import bs4, cloudscraper, random, time, requests\n",
    "\n",
    "# 載入 Selenium 相關模組\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "\n",
    "def tfidf_similarity(s1, s2):\n",
    "    def add_space(s):\n",
    "        return ' '.join(list(s))\n",
    "    # 將字中間加入空格\n",
    "    s1, s2 = add_space(s1), add_space(s2)\n",
    "    # 轉化為TF矩陣\n",
    "    cv = TfidfVectorizer(tokenizer=lambda s: s.split())\n",
    "    corpus = [s1, s2]\n",
    "    vectors = cv.fit_transform(corpus).toarray()\n",
    "    \n",
    "    # 計算TF係數\n",
    "    fiend = np.dot(vectors[0], vectors[1]) / (norm(vectors[0]) * norm(vectors[1])) \n",
    "    np.seterr(invalid='ignore') # 當計算結果為無意義(分母為0)，忽略此警告\n",
    "    return fiend\n",
    "    \n",
    "def stopwordslist(filepath):\n",
    "    stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]\n",
    "    return stopwords\n",
    "\n",
    "def movestopwords(sentence):\n",
    "    stopwords = stopwordslist('stopword.txt')  # 這裏加載停用詞的路徑\n",
    "    outstr = ''\n",
    "    for word in sentence:           \n",
    "        if word not in stopwords:  \n",
    "            if word != '\\t'and'\\n':\n",
    "                outstr += word\n",
    "    return outstr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定Chrome Driver 的執行檔路徑\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--incognito\") # 啟動進入無痕模式\n",
    "chrome_options.add_argument(\"--window-size=1,1\") # 頁面長度寬度調整\n",
    "# chrome_options.add_argument('--headless')  # 啟動Headless 無頭(隱藏瀏覽器)\n",
    "\n",
    "# 隱藏\"Chrome正在受到自動軟體的控制\"\n",
    "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "chrome_options.add_argument('--disable-gpu') #關閉GPU 避免某些系統或是網頁出錯\n",
    "chrome_options.add_argument('--hide-scrollbars')     # 隱藏滾動條, 應對一些特殊頁面\n",
    "chrome_options.chrome_executable_path = \"C:\\\\Users\\\\student\\\\Desktop\\\\model_compar\\\\chromedriver.exe\"\n",
    "\n",
    "def dcardCraw(url):\n",
    "    driver.get(url)\n",
    "    # print(driver.page_source)\n",
    "    data = driver.page_source #取得網頁的原始碼\n",
    "\n",
    "    # 讓beautifulSoup協助我們解析HTML格式文件\n",
    "    root = bs4.BeautifulSoup(data, \"html.parser\")\n",
    "    titles = root.find(\"div\", class_=\"sc-ba53eaa8-0 hKkUKs\")  # 用列表顯示全部爬蟲下來的標題\n",
    "\n",
    "    for title in titles:\n",
    "        result = title.text.strip().replace('\\n', '').replace(' ', '')\n",
    "        #印出內文\n",
    "        print(result)\n",
    "        \n",
    "    driver.close()\n",
    "    \n",
    "    return result\n",
    "    # titles代表div標籤\n",
    "    # 尋找class = \"title\" 的div 標籤，因為class是保留字，所以寫成class_\n",
    "    # root 代表整個網頁、title是網頁標籤也是網頁標題\n",
    "    # cls 是清空終端機(terminal)\n",
    "    # mode = \"a\"是以附加的方式打開並寫入文件，因為mode = \"w\"會將檔案清空在寫入，mode=\"a\"不會清空\n",
    "    \n",
    "def pttCraw(url):\n",
    "    driver.close()\n",
    "    #建立一個Request 物件，附加Request Headers 的資訊\n",
    "    request = req.Request(url, headers={\n",
    "        \"cookie\":\"over18=1\",\n",
    "        \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36\"\n",
    "    })\n",
    "    with req.urlopen(request) as response:\n",
    "        data = response.read().decode(\"utf-8\")\n",
    "\n",
    "    # print(data)\n",
    "    #「解析」原始碼，取得每篇文章的問題\n",
    "    # utf-8(比較省空間)有部分的漢字不能轉換所以要用GB18030編碼\n",
    "\n",
    "    root = bs4.BeautifulSoup(data, \"html.parser\") # 讓beautifulSoup協助我們解析HTML格式文件\n",
    "    titles = root.find(\"div\", class_ = \"bbs-screen bbs-content\").text # 用爬蟲抓內文\n",
    "    \n",
    "    #去除掉 target_content\n",
    "    target_content = '※ 發信站: 批踢踢實業坊(ptt.cc),'\n",
    "    content = titles.split(target_content)\n",
    "    \n",
    "    #去除掉 作者 看板 標題 時間\n",
    "    results = root.select('span.article-meta-value')\n",
    "\n",
    "    if len(results)>3:\n",
    "        #作者 看板 標題 時間\n",
    "        firstLine = \"作者\" + results[0].text + \"看板\" + results[1].text + \"標題\" + results[2].text + \"時間\" + results[3].text\n",
    "\n",
    "    content = content[0].split(firstLine)\n",
    "    \n",
    "    #去除掉文末 --\n",
    "    main_content = content[1].replace('--', '')\n",
    "\n",
    "    #去除掉換行\n",
    "    main_content = main_content.replace('\\n', '')\n",
    "    \n",
    "    #印出內文\n",
    "    print(main_content)\n",
    "    \n",
    "    return main_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### youtube API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoutubeSpider():\n",
    "    def __init__(self, api_key):\n",
    "        self.base_url = \"https://www.googleapis.com/youtube/v3/search?type=video&part=snippet&maxResults=1\"\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def get_html_to_json(self, path):\n",
    "        \"\"\"組合 URL 後 GET 網頁並轉換成 JSON\"\"\"\n",
    "        api_url = f\"{self.base_url}&key={self.api_key}{path}\"\n",
    "        r = requests.get(api_url)\n",
    "        if r.status_code == requests.codes.ok:\n",
    "            data = r.json()\n",
    "        else:\n",
    "            data = None\n",
    "        return data\n",
    "    \n",
    "    def get_ytSearch(self, theKey):\n",
    "        path = f'&q={theKey}'\n",
    "        data = self.get_html_to_json(path)\n",
    "        \n",
    "        try:\n",
    "            uploads_id = data['items'][0]['id']['videoId']\n",
    "            # uploads_id = data #輸出是dict\n",
    "        except KeyError:\n",
    "            uploads_id = None\n",
    "        return uploads_id\n",
    "    \n",
    "YOUTUBE_API_KEY = \"AIzaSyBKdJO0Q7tS8jQyuZUx0kNmgFD2L73Bn1E\"\n",
    "youtube_spider = YoutubeSpider(YOUTUBE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算配適度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('done_2021-08to12.csv')\n",
    "\n",
    "def find_song(url):\n",
    "    if url[12:15] == \"dca\":\n",
    "        article = dcardCraw(url)\n",
    "    else:\n",
    "        article = pttCraw(url)\n",
    "\n",
    "    lyrics=train['lyrics']\n",
    "    i=0\n",
    "    num=0\n",
    "    highpri=0\n",
    "    for text in lyrics:\n",
    "        text=movestopwords(text)\n",
    "        text=text.replace(' ','')\n",
    "        text=text.replace(',','，')\n",
    "        if tfidf_similarity(text, article)>highpri:\n",
    "            highpri=tfidf_similarity(text, article)\n",
    "            num=i\n",
    "        i+=1\n",
    "    \n",
    "    author = train.singer.iloc[num]\n",
    "    songName = train.name.iloc[num]\n",
    "    youtube_theKey = author + ' ' + songName  # 孫凱旋 專屬\n",
    "    uploads_id = youtube_spider.get_ytSearch(youtube_theKey)\n",
    "    \n",
    "    print('配適度:',highpri,'作者:',train.singer.iloc[num],'歌名:',train.name.iloc[num], '情緒:',train.moodCat.iloc[num])\n",
    "    print(\"連結:https://www.youtube.com/watch?v=\"+uploads_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 抓取檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\student\\AppData\\Local\\Temp\\ipykernel_18232\\4011322761.py:2: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome(chrome_options = chrome_options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "你覺得蝴蝶漂亮嗎？蝴蝶的翅膀很美但是牠自己看不到你自己的美可能自己感覺不到而自卑在你欣賞別人的同時別人也欣賞你所以不要自卑好嗎？\n",
      "配適度: 0.3221601152341681 作者: 南西肯恩 歌名: 我想再擁有自己 情緒: 懼\n",
      "連結:https://www.youtube.com/watch?v=vjzKK53-6EI\n"
     ]
    }
   ],
   "source": [
    "#建立 Driver 物件實體，用程式操作瀏覽器運作\n",
    "driver = webdriver.Chrome(chrome_options = chrome_options)\n",
    "driver.minimize_window() #視窗縮小化\n",
    "\n",
    "# article = find_song('https://www.dcard.tw/f/relationship/p/238632575')\n",
    "# article = find_song('https://www.dcard.tw/f/talk/p/239984330')\n",
    "article = find_song('https://www.dcard.tw/f/photography/p/240167044')\n",
    "# article = find_song(\"http://www.ptt.cc/bbs/Boy-Girl/M.1664277279.A.9AA.html\")\n",
    "# article = find_song(\"https://www.ptt.cc/bbs/Gossiping/M.1664530650.A.4E3.html\")\n",
    "# article = find_song(\"http://www.ptt.cc/bbs/Boy-Girl/M.1660356781.A.365.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9bf8e5f6ae5a440c6649c43ab49956741af2ee52909e232ddcd4abcc58504c21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
